{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping using Subreddit APIs\n",
    "\n",
    "In this notebook, I webscrape the following 2 subreddits:\n",
    "1. `r/Conservative`\n",
    "2. `r/democrats`\n",
    "\n",
    "I also view the data on first scrape and perform some preliminary analysis on the data to determine which fields are useful for analysis later on. \n",
    "\n",
    "Contents:\n",
    "- [Imports](#Import-libraries)\n",
    "- [Webscraping Function](#Webscraping-function)\n",
    "- [Scraped data from `r/Conservative`](#Scrape-data-from-`r/Conservative`)\n",
    "- [Data exploration on `r/Conservative` scraped data](#-Data-exploration-on-`r/Conservative`-scraped-data)\n",
    "- [Scraped data from `r/democrats`](#Scrape-data-from-`r/democrats`)\n",
    "- [Data exploration on `r/democrats` scraped data](#Data-exploration-on-`r/democrats`-scraped-data)\n",
    "-[Scraped comments from posts](#Scrape-comments-from-posts)\n",
    "- [Save datasets to csv file](#Save-datasets-to-csv-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "# !pip install praw\n",
    "import praw\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !pip install prettytable\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping function\n",
    "\n",
    "**JSON API**\n",
    "\n",
    "Using json api, I create a function to first scrape the top posts and comments. There is a limit of up to 1,000 reddit posts so we can expect this many posts for each subreddit or lesser if there are duplicate posts in the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit(subreddit, num_posts):\n",
    "    # creates a dataframe of subreddit posts\n",
    "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "    headers = {'User-agent': 'fun-sized 1.0'}\n",
    "    posts_full = []\n",
    "    posts_unique = []\n",
    "    after = None\n",
    "\n",
    "    num_scrape = int(np.ceil(num_posts/25))\n",
    "    for i in range(num_scrape):\n",
    "        \n",
    "        #print tracker\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Scraped {i + 1} pages...')\n",
    "        \n",
    "        #update url for next reddit page after the first\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "\n",
    "        #use request lib to get html, use useragent under headers to avoid 429 status error.\n",
    "        res = requests.get(current_url, headers = headers) \n",
    "\n",
    "        #print error if encountered\n",
    "        if res.status_code != 200:\n",
    "            print('Status error: ', res.status_code) \n",
    "            break\n",
    "\n",
    "        current_dict = res.json() #get the dictionary of data from current 25 posts for subreddit\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']] #extract just the posts for the subreddit\n",
    "        posts_full.extend(current_posts) #append current posts to list of posts previously extracted\n",
    "        after = current_dict['data']['after'] #set after to create new url for the next 25 posts \n",
    "        \n",
    "        #generate random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    #remove duplicates\n",
    "    titles = []   \n",
    "    for p in posts_full:\n",
    "        if p['title'] not in titles: #use title to check for duplicates\n",
    "            titles.append(p['title'])\n",
    "            posts_unique.append(p)\n",
    "    \n",
    "    return pd.DataFrame(posts_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape data from `r/Conservative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 pages...\n",
      "Scraped 20 pages...\n",
      "Scraped 30 pages...\n",
      "Scraped 40 pages...\n",
      "Scraped 50 pages...\n",
      "Scraped 60 pages...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>gallery_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td>t2_4hgotpz8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Parler finds refuge with right-leaning webhost...</td>\n",
       "      <td>[{'e': 'text', 't': 'Flaired Users Only'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.washingtonexaminer.com/news/parler...</td>\n",
       "      <td>632488</td>\n",
       "      <td>1.610414e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td>t2_4hgotpz8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Elon Musk: A lot of people are going to be sup...</td>\n",
       "      <td>[{'e': 'text', 't': 'Flaired Users Only'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://twitchy.com/samj-3930/2021/01/11/hey-g...</td>\n",
       "      <td>632488</td>\n",
       "      <td>1.610397e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td>t2_4u7bd</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Sorry Cleveland</td>\n",
       "      <td>[{'e': 'text', 't': 'Flaired Users Only'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://i.redd.it/ukmkh9mo8ta61.jpg</td>\n",
       "      <td>632488</td>\n",
       "      <td>1.610418e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>3cbf370a-5fe4-11e6-9198-0e0b7c2c3ef3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td>t2_d0rgw</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Democrat Law Professor: Trump Never Actually C...</td>\n",
       "      <td>[{'e': 'text', 't': 'Flaired Users Only'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://townhall.com/tipsheet/katiepavlich/202...</td>\n",
       "      <td>632488</td>\n",
       "      <td>1.610410e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Conservative</td>\n",
       "      <td></td>\n",
       "      <td>t2_4hgotpz8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Elon Musk Advises People to Ditch Facebook and...</td>\n",
       "      <td>[{'e': 'text', 't': 'Flaired Users Only'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.digitaltrends.com/news/elon-musk-f...</td>\n",
       "      <td>632488</td>\n",
       "      <td>1.610390e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc     subreddit selftext author_fullname  saved  \\\n",
       "0            None  Conservative              t2_4hgotpz8  False   \n",
       "1            None  Conservative              t2_4hgotpz8  False   \n",
       "2            None  Conservative                 t2_4u7bd  False   \n",
       "3            None  Conservative                 t2_d0rgw  False   \n",
       "4            None  Conservative              t2_4hgotpz8  False   \n",
       "\n",
       "  mod_reason_title  gilded  clicked  \\\n",
       "0             None       0    False   \n",
       "1             None       0    False   \n",
       "2             None       0    False   \n",
       "3             None       0    False   \n",
       "4             None       0    False   \n",
       "\n",
       "                                               title  \\\n",
       "0  Parler finds refuge with right-leaning webhost...   \n",
       "1  Elon Musk: A lot of people are going to be sup...   \n",
       "2                                    Sorry Cleveland   \n",
       "3  Democrat Law Professor: Trump Never Actually C...   \n",
       "4  Elon Musk Advises People to Ditch Facebook and...   \n",
       "\n",
       "                          link_flair_richtext  ...  \\\n",
       "0  [{'e': 'text', 't': 'Flaired Users Only'}]  ...   \n",
       "1  [{'e': 'text', 't': 'Flaired Users Only'}]  ...   \n",
       "2  [{'e': 'text', 't': 'Flaired Users Only'}]  ...   \n",
       "3  [{'e': 'text', 't': 'Flaired Users Only'}]  ...   \n",
       "4  [{'e': 'text', 't': 'Flaired Users Only'}]  ...   \n",
       "\n",
       "                                                 url  subreddit_subscribers  \\\n",
       "0  https://www.washingtonexaminer.com/news/parler...                 632488   \n",
       "1  https://twitchy.com/samj-3930/2021/01/11/hey-g...                 632488   \n",
       "2                https://i.redd.it/ukmkh9mo8ta61.jpg                 632488   \n",
       "3  https://townhall.com/tipsheet/katiepavlich/202...                 632488   \n",
       "4  https://www.digitaltrends.com/news/elon-musk-f...                 632488   \n",
       "\n",
       "    created_utc num_crossposts  media  is_video  \\\n",
       "0  1.610414e+09              0   None     False   \n",
       "1  1.610397e+09              1   None     False   \n",
       "2  1.610418e+09              0   None     False   \n",
       "3  1.610410e+09              2   None     False   \n",
       "4  1.610390e+09              0   None     False   \n",
       "\n",
       "                 link_flair_template_id  is_gallery media_metadata  \\\n",
       "0                                   NaN         NaN            NaN   \n",
       "1                                   NaN         NaN            NaN   \n",
       "2  3cbf370a-5fe4-11e6-9198-0e0b7c2c3ef3         NaN            NaN   \n",
       "3                                   NaN         NaN            NaN   \n",
       "4                                   NaN         NaN            NaN   \n",
       "\n",
       "   gallery_data  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape 1,500 posts from r/Conservative\n",
    "conservative_df = scrape_subreddit('Conservative', 1500)\n",
    "conservative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491, 111)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conservative_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see 491 unique titles out of about 1,000 posts on the subreddit, which is slightly less than half. More data would be better to work with and this will be considered in further analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration on `r/Conservative` scraped data\n",
    "I do a quick review of the dataset using pandas profiling report, to identify which fields would be useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3760ac9d09af4863ae58e6b60630bdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68c8438e93842e5bf820e2266359d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e9abc6567d49a7a262116f00d4f1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render HTML'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16fe703494d47d78cd147ca2a7981fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Export report to file'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report is ready!\n"
     ]
    }
   ],
   "source": [
    "# generate html summary report about data - r/democrats\n",
    "report = ProfileReport(conservative_df)\n",
    "report.to_file(output_file = 'report_conservative_df.html')\n",
    "print(\"Report is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the conservative dataframe report, I note the following:\n",
    "1. 'selftext' column which contains the content of the reddit posts are empty in this subreddit. Looking through the subreddit `r/Conservative`, each post mostly usually contains title and a news link and rarely a text. While this would be most useful for modelling, there is no data. An alternative would be using comments data, which I will scrape later on. \n",
    "\n",
    "\n",
    "2. 'title' column has no missing value. Apart from the content of the posts, the title is probably the closest identifier of the post to a particular subreddit. This would be my first option for modelling.\n",
    "\n",
    "\n",
    "3. 'link_flair_text' column has 9 missing data (2.2% of dataset). Flair in reddit is used as a form of subcategorisation within the subreddit (Mathew, 2019). There are three types of flairs for posts : Flaired Users Only, Satire - Flaired Users Only and Misleading Title. These are not very useful for modelling later so we will not consider this. \n",
    "\n",
    "\n",
    "4. 'domain' shows the source of the link eg. foxnews, dailywire, washingtonexaminer etc. There are no missing values under this column so this could be considered for modelling later on. This is especially since for instance, foxnews is well-known to be controlled by Republicans so we are likely to see more links in this subreddit from this news channel. \n",
    "\n",
    "\n",
    "5. 'subreddit' column will be the label for our classification model later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape data from `r/democrats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 pages...\n",
      "Scraped 20 pages...\n",
      "Scraped 30 pages...\n",
      "Scraped 40 pages...\n",
      "Scraped 50 pages...\n",
      "Scraped 60 pages...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>is_gallery</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>gallery_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>democrats</td>\n",
       "      <td></td>\n",
       "      <td>t2_nkk56</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>House Democrats launch second impeachment of T...</td>\n",
       "      <td>[{'e': 'text', 't': 'üî¥ Megathread'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/JoeBiden/comments/kv4jen/house_democrats_la...</td>\n",
       "      <td>175113</td>\n",
       "      <td>1.610378e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>democrats</td>\n",
       "      <td></td>\n",
       "      <td>t2_4f8e5viq</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Do I have to?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://i.redd.it/sq8b597wpsa61.jpg</td>\n",
       "      <td>175113</td>\n",
       "      <td>1.610411e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>democrats</td>\n",
       "      <td></td>\n",
       "      <td>t2_17c1os</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Camp Auschwitz\" guy identified!</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://i.redd.it/ewmxi3oh0qa61.jpg</td>\n",
       "      <td>175113</td>\n",
       "      <td>1.610378e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>democrats</td>\n",
       "      <td></td>\n",
       "      <td>t2_yfff4</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>No Crawling Back!!!</td>\n",
       "      <td>[{'e': 'text', 't': 'üìÑEffortpost'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://i.redd.it/863qmtjcbsa61.jpg</td>\n",
       "      <td>175113</td>\n",
       "      <td>1.610406e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>democrats</td>\n",
       "      <td></td>\n",
       "      <td>t2_2tm5mq8b</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Use the 14th Amendment to ban Trump</td>\n",
       "      <td>[{'e': 'text', 't': 'üó≥Ô∏è Beat Trump'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>https://i.redd.it/nil93j0cmsa61.jpg</td>\n",
       "      <td>175113</td>\n",
       "      <td>1.610410e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc  subreddit selftext author_fullname  saved mod_reason_title  \\\n",
       "0            None  democrats                 t2_nkk56  False             None   \n",
       "1            None  democrats              t2_4f8e5viq  False             None   \n",
       "2            None  democrats                t2_17c1os  False             None   \n",
       "3            None  democrats                 t2_yfff4  False             None   \n",
       "4            None  democrats              t2_2tm5mq8b  False             None   \n",
       "\n",
       "   gilded  clicked                                              title  \\\n",
       "0       0    False  House Democrats launch second impeachment of T...   \n",
       "1       0    False                                      Do I have to?   \n",
       "2       0    False                   \"Camp Auschwitz\" guy identified!   \n",
       "3       0    False                                No Crawling Back!!!   \n",
       "4       0    False                Use the 14th Amendment to ban Trump   \n",
       "\n",
       "                     link_flair_richtext  ...  \\\n",
       "0   [{'e': 'text', 't': 'üî¥ Megathread'}]  ...   \n",
       "1                                     []  ...   \n",
       "2                                     []  ...   \n",
       "3    [{'e': 'text', 't': 'üìÑEffortpost'}]  ...   \n",
       "4  [{'e': 'text', 't': 'üó≥Ô∏è Beat Trump'}]  ...   \n",
       "\n",
       "                                                 url  subreddit_subscribers  \\\n",
       "0  /r/JoeBiden/comments/kv4jen/house_democrats_la...                 175113   \n",
       "1                https://i.redd.it/sq8b597wpsa61.jpg                 175113   \n",
       "2                https://i.redd.it/ewmxi3oh0qa61.jpg                 175113   \n",
       "3                https://i.redd.it/863qmtjcbsa61.jpg                 175113   \n",
       "4                https://i.redd.it/nil93j0cmsa61.jpg                 175113   \n",
       "\n",
       "    created_utc num_crossposts  media  is_video author_cakeday  is_gallery  \\\n",
       "0  1.610378e+09              0   None     False            NaN         NaN   \n",
       "1  1.610411e+09              1   None     False            NaN         NaN   \n",
       "2  1.610378e+09              2   None     False            NaN         NaN   \n",
       "3  1.610406e+09              0   None     False            NaN         NaN   \n",
       "4  1.610410e+09              0   None     False            NaN         NaN   \n",
       "\n",
       "  media_metadata  gallery_data  \n",
       "0            NaN           NaN  \n",
       "1            NaN           NaN  \n",
       "2            NaN           NaN  \n",
       "3            NaN           NaN  \n",
       "4            NaN           NaN  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape 1,500 posts from r/democrats\n",
    "democrats_df = scrape_subreddit('democrats', 1500)\n",
    "democrats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 114)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "democrats_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration on `r/democrats` scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec0d60acbd0455896181d04c8c192f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2awan\\anaconda3\\lib\\site-packages\\pandas_profiling\\model\\correlations.py:101: UserWarning: There was an attempt to calculate the cramers correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"cramers\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/pandas-profiling/pandas-profiling/issues\n",
      "(include the error message: 'No data; `observed` has size 0.')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad673dd2ad9c473494b0804375d90870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204f3818c05d4c70b8eb2a79ccb98861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render HTML'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3312b8badd8b42efb89092056098d126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Export report to file'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report is ready!\n"
     ]
    }
   ],
   "source": [
    "# generate html summary report about data - r/democrats\n",
    "report = ProfileReport(democrats_df)\n",
    "report.to_file(output_file = 'report_democrats_df.html')\n",
    "print(\"Report is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the democrats dataframe report, there are some similar characteristics with the conservatives dataframe profile report. Below the details:\n",
    "1. 94% of the 'selftext' column is empty. Similar to the subreddit `r/Conservative`, each post mostly usually contains title and a news link and rarely a text. While this would be most useful for modelling, there is no data. An alternative would be using comments data, which I will scrape later on. \n",
    "\n",
    "\n",
    "2. 'title' column has no missing value. Similar to `r/Conservative`, this would be the first option to use for modelling since it closely reflects the content of the post.\n",
    "\n",
    "\n",
    "3. 'link_flair_text' column has 45.4% missing data which is pretty substantial. I would not consider this for modelling.\n",
    "\n",
    "\n",
    "4. 'domain' shows the source of the link eg. self.democrats, twitter, youtube etc. There are no missing values under this column so this could be considered for modelling as well.\n",
    "\n",
    "\n",
    "5. 'subreddit' column will be the label for our classification model later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape comments from posts\n",
    "From analysis above, I decided to scrape comments as an alternative to post content, since post content is lacking in the subreddits selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_comments(subreddit):\n",
    "    #function scrapes the top 10 comments from the top 10 posts in the subreddit\n",
    "    url = f'https://www.reddit.com/r/{subreddit}/top.json'\n",
    "    headers = {'User-agent': 'fun-sized 2.0'}\n",
    "    \n",
    "    res = requests.get(url, headers = headers)\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        raw_dict = res.json() # get dict of data from top 25 posts for subreddit\n",
    "        comments = []\n",
    "\n",
    "        for i in range(24):\n",
    "            permalink = raw_dict['data']['children'][i]['data']['permalink'] \n",
    "            comment_url = f'https://www.reddit.com{permalink}.json?sort=confidence' #get the url for comments of individual post\n",
    "            res_comment = requests.get(comment_url, headers = headers)\n",
    "            if res_comment.status_code == 200:\n",
    "                dict_best_comments = res_comment.json()\n",
    "                indiv_comments = dict_best_comments[1]['data']['children']\n",
    "                for j in range(100):\n",
    "                    try:\n",
    "                        comments.append(indiv_comments[j]['data']['body'])\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "        comments_df = pd.DataFrame(comments, columns = ['comments'])\n",
    "        comments_df['subreddit'] = [f'{subreddit}' for k in range(len(comments_df))]\n",
    "    \n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for debate? Head to the public section...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Except their lawyers have dropped Parler as a ...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Republicans had six years to do something abou...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/r/news has been celebrating all day. They lov...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wish him luck but what can he do at this poi...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments     subreddit\n",
       "0  Looking for debate? Head to the public section...  Conservative\n",
       "1  Except their lawyers have dropped Parler as a ...  Conservative\n",
       "2  Republicans had six years to do something abou...  Conservative\n",
       "3  /r/news has been celebrating all day. They lov...  Conservative\n",
       "4  I wish him luck but what can he do at this poi...  Conservative"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#scrape comments from r/Conservative\n",
    "conservative_top_comments_df = scrape_top_comments('Conservative')\n",
    "conservative_top_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(729, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view number of comments\n",
    "conservative_top_comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that there are 729 comments scraped from `r/Conservative`, which is a decent amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My hero!! He made these idiots to go the other...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Isn‚Äôt this the Blue Lives Matter/Law &amp;amp; Ord...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ability to make critical decisions on the spot.</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goodman is good man!\\n\\nSorry I had too</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He is the only real American in that damn room...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  subreddit\n",
       "0  My hero!! He made these idiots to go the other...  democrats\n",
       "1  Isn‚Äôt this the Blue Lives Matter/Law &amp; Ord...  democrats\n",
       "2    Ability to make critical decisions on the spot.  democrats\n",
       "3            Goodman is good man!\\n\\nSorry I had too  democrats\n",
       "4  He is the only real American in that damn room...  democrats"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "democrats_top_comments_df = scrape_top_comments('democrats')\n",
    "democrats_top_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "democrats_top_comments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that there are 262 comments scraped from `r/democrats`, which is quite lacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Reddit API Wrapper(PRAW)**\n",
    "\n",
    "\n",
    "Extracting titles and comments can also be done with Python Reddit API Wrapper(PRAW) (Boe, 2020), which is faster than json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='bxZEn1eDscHbUg', client_secret='TK1X8gY_fS-TjHTz61H156z6uKOkcQ', user_agent='reddit-webscrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that scrape titles of subreddit\n",
    "def praw_scrape_titles(subreddit, num_post):\n",
    "    post_title = []\n",
    "    top_posts_pols = reddit.subreddit(subreddit).hot(limit=num_post)\n",
    "    for post in top_posts_pols:\n",
    "        post_title.append(post.title)\n",
    "    \n",
    "    praw_titles_df = pd.DataFrame(post_title, columns = ['titles'])\n",
    "    praw_titles_df['subreddit'] = [f'{subreddit}' for k in range(len(praw_titles_df))]   \n",
    "    \n",
    "    return praw_titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter‚Äôs ban on Trump strips US of ‚Äòmoral hig...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parler finds refuge with right-leaning webhost...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry Cleveland</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elon Musk: A lot of people are going to be sup...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrat Law Professor: Trump Never Actually C...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles     subreddit\n",
       "0  Twitter‚Äôs ban on Trump strips US of ‚Äòmoral hig...  Conservative\n",
       "1  Parler finds refuge with right-leaning webhost...  Conservative\n",
       "2                                    Sorry Cleveland  Conservative\n",
       "3  Elon Musk: A lot of people are going to be sup...  Conservative\n",
       "4  Democrat Law Professor: Trump Never Actually C...  Conservative"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape titles off r/Conservative using praw\n",
    "cons_praw_titles_df = praw_scrape_titles('Conservative', 1000)\n",
    "cons_praw_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cons_praw_titles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully scraped 504 titles off `r/Conservative` using praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Democrats launch second impeachment of T...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do I have to?</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Camp Auschwitz\" guy identified!</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Crawling Back!!!</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use the 14th Amendment to ban Trump</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  subreddit\n",
       "0  House Democrats launch second impeachment of T...  democrats\n",
       "1                                      Do I have to?  democrats\n",
       "2                   \"Camp Auschwitz\" guy identified!  democrats\n",
       "3                                No Crawling Back!!!  democrats\n",
       "4                Use the 14th Amendment to ban Trump  democrats"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape titles off r/democrats using praw\n",
    "dems_praw_titles_df = praw_scrape_titles('democrats', 1000)\n",
    "dems_praw_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dems_praw_titles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully scraped 997 titles off `r/democrats` using praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that scrapes top comments from each subreddit\n",
    "def praw_scrape_comments(subreddit):\n",
    "    url = f'https://www.reddit.com/r/{subreddit}/top.json'\n",
    "    headers = {'User-agent': 'fun-sized 3.0'}\n",
    "    res = requests.get(url, headers = headers)\n",
    "    if res.status_code == 200:\n",
    "        raw_dict = res.json()\n",
    "        comments = []\n",
    "        for i in range(24):\n",
    "            permalink = raw_dict['data']['children'][i]['data']['permalink']\n",
    "            comment_url = f'https://www.reddit.com{permalink}.json?sort=confidence'\n",
    "            submission = reddit.submission(url = comment_url)\n",
    "            submission.comments.replace_more(limit = None)\n",
    "            for comment in submission.comments.list():\n",
    "                text =str(comment.body)\n",
    "                comments.append(text)                         \n",
    "    \n",
    "        comments_df = pd.DataFrame(comments, columns = ['comments'])\n",
    "        comments_df['subreddit'] = [f'{subreddit}' for k in range(len(comments_df))]   \n",
    "    \n",
    "    return comments_df       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for debate? Head to the public section...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Except their lawyers have dropped Parler as a ...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Republicans had six years to do something abou...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/r/news has been celebrating all day. They lov...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wish him luck but what can he do at this poi...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments     subreddit\n",
       "0  Looking for debate? Head to the public section...  Conservative\n",
       "1  Except their lawyers have dropped Parler as a ...  Conservative\n",
       "2  Republicans had six years to do something abou...  Conservative\n",
       "3  /r/news has been celebrating all day. They lov...  Conservative\n",
       "4  I wish him luck but what can he do at this poi...  Conservative"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#scrape comments off r/Conservative using praw\n",
    "cons_praw_comments_df = praw_scrape_comments('Conservative')\n",
    "cons_praw_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4248"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cons_praw_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Successfully scraped 4248 comments off `r/Conservative` using praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My hero!! He made these idiots to go the other...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Isn‚Äôt this the Blue Lives Matter/Law &amp; Order c...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ability to make critical decisions on the spot.</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goodman is good man!\\n\\nSorry I had too</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He is the only real American in that damn room...</td>\n",
       "      <td>democrats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  subreddit\n",
       "0  My hero!! He made these idiots to go the other...  democrats\n",
       "1  Isn‚Äôt this the Blue Lives Matter/Law & Order c...  democrats\n",
       "2    Ability to make critical decisions on the spot.  democrats\n",
       "3            Goodman is good man!\\n\\nSorry I had too  democrats\n",
       "4  He is the only real American in that damn room...  democrats"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#scrape comments off r/democrats using praw\n",
    "dems_praw_comments_df = praw_scrape_comments('democrats')\n",
    "dems_praw_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dems_praw_comments_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully scraped 573 titles off `r/democrats` using praw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------------+----------------------------+\n",
      "|   Mode and Subreddit  | Number of Titles Scraped | Number of Comments Scraped |\n",
      "+-----------------------+--------------------------+----------------------------+\n",
      "| JSON | r/Conservative |           491            |            729             |\n",
      "| PRAW | r/Conservative |           504            |            4248            |\n",
      "|   JSON | r/democrats  |           990            |            262             |\n",
      "|   PRAW | r/democrats  |           997            |            573             |\n",
      "+-----------------------+--------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "# view number of data collected in table \n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Mode and Subreddit\", \"Number of Titles Scraped\", \"Number of Comments Scraped\"]\n",
    "\n",
    "x.add_row([\"JSON | r/Conservative\", len(conservative_df), len(conservative_top_comments_df)])\n",
    "x.add_row([\"PRAW | r/Conservative\", len(cons_praw_titles_df), len(cons_praw_comments_df)])\n",
    "x.add_row([\"JSON | r/democrats\", len(democrats_df), len(democrats_top_comments_df)])\n",
    "x.add_row([\"PRAW | r/democrats\", len(dems_praw_titles_df), len(dems_praw_comments_df)])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally `r/democrats` had more titles (possibly because there are duplicate titles under `r/Conservative` where several or the same user(s) share the same title and newslink to get more redditors' attention on the newslink). Conversely, there are much more comments on `r/Conservative` than for `r/democrats`. Also, praw was able to scrape all sub-comments, resulting in much more data available for comments. \n",
    "\n",
    "We will move forward with praw-scraped datasets as overall it has more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save datasets to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate title dataframes and save to csv\n",
    "titles_df = pd.concat([cons_praw_titles_df, dems_praw_titles_df])\n",
    "titles_df.to_csv('../data/titles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate comments dataframes and save to csv\n",
    "comments_df = pd.concat([cons_praw_comments_df, dems_praw_comments_df])\n",
    "comments_df.to_csv('../data/comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be performing data cleaning and exploratory data analysis on these datasets in [this notebook](02-data-cleaning-and-preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\"PRAW: The Python Reddit API Wrapper\"(Boe, 2020)\n",
    "https://praw.readthedocs.io/en/latest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
